import math
import transformers
import torch
import torch.nn.functional as F
from lm_eval.base import BaseLM
from lm_eval import utils
from tqdm import tqdm
import numpy as np
from transformers.generation import LogitsWarper, LogitsProcessorList
import os

from typing import List


class CFGLogits(LogitsWarper):

    def __init__(self, cfg, inputs, model):
        self.cfg = cfg
        self.inputs = inputs
        self.model = model
        self.out = None

    def __call__(self, input_ids, scores):
        if self.cfg == 1:
            return F.log_softmax(scores, dim=-1)
        scores = F.log_softmax(scores, dim=-1)
        if self.out is None:
            self.out = self.model(self.inputs, use_cache=True)
        else:
            self.out = self.model(input_ids[:, -1:],
                                  use_cache=True,
                                  past_key_values=self.out.past_key_values)
        unconditional_logits = F.log_softmax(self.out.logits[0][-1:], dim=-1)
        out = self.cfg * (scores - unconditional_logits) + unconditional_logits
        return out


class T5LM(BaseLM):
    def __init__(
        self, device="cuda", pretrained="google/t5-small-lm-adapt", batch_size=1
    ):
        super().__init__()
        if device:
            self._device = torch.device(device)
            print(f"Using device '{device}'")
        else:
            print("Device not specified")
            print(f"Cuda Available? {torch.cuda.is_available()}")
            self._device = (
                torch.device("cuda")
                if torch.cuda.is_available()
                else torch.device("cpu")
            )

        #self.t5 = transformers.AutoModelForSeq2SeqLM.from_pretrained(pretrained).to(
        #    self.device
        #)
        self.t5 = transformers.AutoModelForSeq2SeqLM.from_pretrained(pretrained, device_map='balanced')
        self.t5.eval()

        self.tokenizer = transformers.T5TokenizerFast.from_pretrained(pretrained)
        self.vocab_size = self.tokenizer.vocab_size

        self.batch_size_per_gpu = int(batch_size)  # * gpus

    @property
    def eot_token(self):
        return self.tokenizer.eos_token

    @property
    def eot_token_id(self):
        return self.tokenizer.eos_token_id

    @property
    def max_length(self):
        # TODO: Add constructor arg for optional `max_length`.
        return 512

    @property
    def max_gen_toks(self):
        # TODO: Add constructor arg for optional `max_gen_toks`.
        return 256

    @property
    def batch_size(self):
        # TODO: fix multi-gpu
        return self.batch_size_per_gpu  # * gpus

    @property
    def device(self):
        return self._device

    def tok_encode(self, string, add_special_tokens=True):
        return self.tokenizer.encode(string, add_special_tokens=add_special_tokens)

    def tok_encode_batch(self, strings, add_special_tokens=True):
        return self.tokenizer(
            strings,
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
            add_special_tokens=add_special_tokens,
        )

    def tok_decode(self, string: str):
        return self.tokenizer.decode(string, skip_special_tokens=True)

    def loglikelihood(self, requests):
        new_requests = []
        for chunk in utils.chunks(requests, self.batch_size):
            context, continuation = zip(*chunk)
            context, continuation = list(context), list(continuation)
            # Fill empty contexts with the EOT token.
            context = [
                f"{self.eot_token}" if len(text) == 0 else text for text in context
            ]
            context_enc = self.tok_encode_batch(context)
            continuation_enc = self.tok_encode_batch(continuation)
            new_requests.append(
                ((context, continuation), context_enc, continuation_enc)
            )
        return self._loglikelihood_tokens(new_requests)

    def loglikelihood_rolling(self, requests):
        with torch.no_grad():
            loglikelihoods = []
            for (string,) in tqdm(requests):
                rolling_token_windows = list(
                    map(
                        utils.make_disjoint_window,
                        utils.get_rolling_token_windows(
                            token_list=self.tok_encode(string),
                            prefix_token=self.eot_token_id,
                            max_seq_len=self.max_length,
                            context_len=1,
                        ),
                    )
                )
                # Split context and continuation windows.
                contexts, continuations = zip(*rolling_token_windows)
                contexts, continuations = list(contexts), list(continuations)

                # Pad the context and continuation windows to form an unragged batch
                # as expected by the `self._model_call()`.
                context_enc = self.tokenizer.pad(
                    {"input_ids": contexts},
                    padding=True,
                    max_length=self.max_length,
                    return_attention_mask=True,
                )
                context_enc = transformers.tokenization_utils_base.BatchEncoding(
                    {k: torch.Tensor(v).long() for k, v in context_enc.items()}
                )
                continuation_enc = self.tokenizer.pad(
                    {"input_ids": continuations},
                    padding=True,
                    max_length=self.max_gen_toks,
                    return_attention_mask=True,
                )
                continuation_enc = transformers.tokenization_utils_base.BatchEncoding(
                    {k: torch.Tensor(v).long() for k, v in continuation_enc.items()}
                )
                # TODO: Extract out this call so it only gets called once and also
                # somehow figure out partial caching for.
                rolling_token_windows_request = [
                    ((contexts, continuations), context_enc, continuation_enc)
                ]
                string_nll = self._loglikelihood_tokens(
                    rolling_token_windows_request, disable_tqdm=True
                )
                string_nll = [x[0] for x in string_nll]  # discard is_greedy
                string_nll = sum(string_nll)
                loglikelihoods.append(string_nll)
        return loglikelihoods

    def _loglikelihood_tokens(self, requests, disable_tqdm=False):
        results = []
        CFG = float(os.environ['CFG'])
        for chunk in tqdm(
            requests, total=math.ceil(len(requests)), disable=disable_tqdm
        ):
            cache_keys, inputs_tokens, targets_tokens = chunk
            #inputs_tokens = inputs_tokens.to(self.device)
            #targets_tokens = targets_tokens.to(self.device)
            inputs_tokens = inputs_tokens
            targets_tokens = targets_tokens
            outputs = self._model_call(inputs_tokens, labels=targets_tokens)
            uncond = self._model_call({"input_ids": inputs_tokens["input_ids"][:, -1:], "attention_mask": inputs_tokens["attention_mask"][:, -1:]}, labels=targets_tokens)  # well, I know the tokens are left-padded. But starting with unattended token seems to be even better for our case.
            #print(outputs.logits.shape)
            #print(uncond.logits.shape)
            log_softmaxes = F.log_softmax(outputs.logits * CFG + uncond.logits * (1 - CFG), dim=-1)

            output_iterator = zip(
                zip(cache_keys[0], cache_keys[1]),
                log_softmaxes,
                targets_tokens["input_ids"],
                targets_tokens["attention_mask"],
            )
            for cache_key, log_softmax, target_tokens, target_mask in output_iterator:
                length = target_mask.sum()
                log_softmax = log_softmax[:length]
                target_tokens = target_tokens[:length]

                greedy_tokens = log_softmax.argmax(dim=-1)
                max_equal = (greedy_tokens == target_tokens).all()

                target_logits = torch.gather(
                    log_softmax, 1, target_tokens.unsqueeze(-1)
                ).squeeze(-1)

                answer = (float(target_logits.sum()), bool(max_equal))
                results.append(answer)

                if cache_key is not None:
                    self.cache_hook.add_partial("loglikelihood", cache_key, answer)
        return results

    def greedy_until(self, requests):
        def _collate(x):
            tokens = self.tok_encode(x[0])
            return len(tokens), x[0]

        results = []
        reorder = utils.Reorderer(requests, _collate)
        for context, until in tqdm(reorder.get_reordered(), disable=False):
            primary_until = until[0]
            context = self.tokenizer(context, return_tensors="pt")
            generation = self._model_generate(
                context,
                max_length=self.max_gen_toks,
                stop=[self.eot_token, primary_until],
            ).tolist()[0]

            s = self.tok_decode(generation)
            for term in until:
                s = s.split(term)[0]
            results.append(s)

            self.cache_hook.add_partial("greedy_until", (context, until), s)
        return reorder.get_original(results)

    def _model_generate(self, context, max_length, stop):
        with torch.no_grad():
            cfg = float(os.environ['CFG'])

            res = self.t5.generate(
                **context,
                max_length=max_length,
                stopping_criteria=stop_sequences_criteria(self.tokenizer, stop),
                do_sample=False,
                use_cache=True,
                logits_processor=LogitsProcessorList(
                    [CFGLogits(cfg, context["input_ids"], self.t5)]),
            )
        return res

    def _model_call(self, inputs, labels):
        with torch.no_grad():
            res = self.t5(**inputs, labels=labels["input_ids"] if labels else None)
        return res


class MultiTokenEOSCriteria(transformers.StoppingCriteria):
    """Criteria to stop on the specified multi-token sequence."""

    def __init__(self, sequence: str, tokenizer: transformers.PreTrainedTokenizer):
        self.sequence = sequence
        self.sequence_id = tokenizer.encode(sequence)
        self.sequence_id_len = len(self.sequence_id) + 1
        self.tokenizer = tokenizer

    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
    ) -> bool:
        last_token_id = input_ids[0, -self.sequence_id_len :]
        last_tokens = self.tokenizer.decode(last_token_id)
        is_stopped = self.sequence in last_tokens
        return is_stopped


def stop_sequences_criteria(
    tokenizer: transformers.PreTrainedTokenizer, stop_sequences: List[str]
) -> transformers.StoppingCriteriaList:
    return transformers.StoppingCriteriaList(
        [
            *[
                MultiTokenEOSCriteria(sequence, tokenizer)
                for sequence in stop_sequences
            ],
        ]
    )
